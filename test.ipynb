{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80600908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional, List\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import threading\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4331668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(\n",
    "    model_path: str,\n",
    "    device: str = \"cuda\"\n",
    ") -> Tuple[AutoModelForCausalLM, AutoTokenizer, Optional[object]]:\n",
    "    \"\"\"\n",
    "    Load a causal language model and its tokenizer.\n",
    "\n",
    "    Args:\n",
    "        model_path: Identifier or local path of the pretrained model.\n",
    "        device: Compute device for inference, e.g. 'cuda' or 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        model: The loaded model in evaluation mode.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "    \"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path\n",
    "    )\n",
    "\n",
    "    model.eval()  # Set model to inference mode\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def test_text_model_streaming(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    prompts: List[str]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run a series of prompts through the model and print out responses token-by-token.\n",
    "\n",
    "    Args:\n",
    "        model: The language model for text generation.\n",
    "        tokenizer: Tokenizer matching the model.\n",
    "        prompts: List of user prompt strings.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Text Model Streaming Test ===\\n\")\n",
    "\n",
    "    for idx, prompt in enumerate(prompts, start=1):\n",
    "        print(f\"Test #{idx}: {prompt}\")\n",
    "        print(\"Response: \", end=\"\", flush=True)\n",
    "\n",
    "        # Prepare chat-style input\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted_input = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        inputs = tokenizer(formatted_input, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Set up the streamer\n",
    "        streamer = TextIteratorStreamer(\n",
    "            tokenizer,\n",
    "            skip_prompt=True,\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # Kick off generation in a background thread\n",
    "        generation_kwargs = dict(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.2,\n",
    "            # do_sample=False,\n",
    "            repetition_penalty=1.1,\n",
    "            streamer=streamer\n",
    "        )\n",
    "        thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "        thread.start()\n",
    "\n",
    "        # Print tokens as they arrive\n",
    "        for chunk in streamer:\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "\n",
    "        thread.join()\n",
    "        print(\"\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5563e2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 'output/fft_20250722_020908'\n",
    "model_path = 'Qwen/Qwen3-0.6B'\n",
    "device = 'cuda:0'\n",
    "\n",
    "# 모델 경로 확인\n",
    "if not os.path.exists(model_path):\n",
    "    warnings.warn(f\"Warning: 로컬 모델 경로 '{model_path}'가 존재하지 않습니다. HuggingFace를 탐색합니다.\")\n",
    "    # sys.exit(1)\n",
    "\n",
    "# 디바이스 설정\n",
    "if device == \"cuda\" and not torch.cuda.is_available():\n",
    "    print(\"CUDA를 사용할 수 없습니다. CPU로 전환합니다.\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"모델 로딩 중: {model_path} (디바이스: {device})\")\n",
    "\n",
    "# 모델 로드\n",
    "model, tokenizer = load_model_and_tokenizer(model_path, device)\n",
    "print(f\"✅ 모델 로딩 완료!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e13a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    \"안녕하세요! 오늘 날씨가 어떤가요?\",\n",
    "    \"안녕하세요! 오늘 날씨가 어떤가요?\",\n",
    "    \"안녕하세요! 자기소개를 해주세요.\",\n",
    "    \"파이썬에서 리스트와 튜플의 차이점은 무엇인가요?\",\n",
    "    \"머신러닝과 딥러닝의 차이를 설명해주세요.\",\n",
    "    \"한국의 전통 음식 3가지를 추천해주세요.\"\n",
    "]\n",
    "\n",
    "test_text_model_streaming(model, tokenizer, test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c89965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
